{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from rl_agents import AlphaBlokusNet\n",
    "from training.train_loop import train_alphablokus\n",
    "from training.evaluator import evaluate_vs_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AlphaBlokusNet()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_alphablokus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtotal_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mgames_per_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mn_mcts_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrain_steps_per_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreplay_buffer_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     16\u001b[0m avg_score \u001b[38;5;241m=\u001b[39m evaluate_vs_random(model, n_games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/git/AlphaBlokus/training/train_loop.py:42\u001b[0m, in \u001b[0;36mtrain_alphablokus\u001b[0;34m(model, device, total_iterations, games_per_iteration, n_mcts_sim, batch_size, train_steps_per_iter, replay_buffer_max, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 1) Self-play\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 42\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_self_play_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgames_per_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mn_mcts_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mcts_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples from self-play.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39mpush(samples)\n",
      "File \u001b[0;32m~/git/AlphaBlokus/training/self_play.py:137\u001b[0m, in \u001b[0;36mgenerate_self_play_data\u001b[0;34m(model, n_games, n_mcts_sim, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_games):\n\u001b[0;32m--> 137\u001b[0m     game_samples \u001b[38;5;241m=\u001b[39m \u001b[43mrun_self_play_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mcts_sim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mcts_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mextend(game_samples)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n",
      "File \u001b[0;32m~/git/AlphaBlokus/training/self_play.py:35\u001b[0m, in \u001b[0;36mrun_self_play_game\u001b[0;34m(model, n_mcts_sim, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminal(st):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# run MCTS from root\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mmcts_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mcts_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_puct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# gather visit counts for each child => policy distribution\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# We sum visits for each piece_id (and pass) across placements\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     visits \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/git/AlphaBlokus/rl_agents/mcts.py:74\u001b[0m, in \u001b[0;36mmcts_search\u001b[0;34m(root, model, n_simulations, c_puct, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m     expand_node(node, model, device)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 3) Evaluate\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 4) Backup\u001b[39;00m\n\u001b[1;32m     76\u001b[0m backup_value(node, value)\n",
      "File \u001b[0;32m~/git/AlphaBlokus/rl_agents/mcts.py:162\u001b[0m, in \u001b[0;36mevaluate_node\u001b[0;34m(node, model, device)\u001b[0m\n\u001b[1;32m    159\u001b[0m st \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal(st):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# compute final score\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     final_scores \u001b[38;5;241m=\u001b[39m \u001b[43mstt_final_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# vantage point is st.current_player at the moment the game ended, though \u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# in a multi-player game, you might define a different scheme. \u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# We'll do a naive approach: the \"value\" is final_scores[current_player] \u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# relative to others, or just normalized. Up to you.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# For simplicity, let's do final_scores[current_player] / 100 \u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# (arbitrary scaling).\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     val \u001b[38;5;241m=\u001b[39m final_scores[st\u001b[38;5;241m.\u001b[39mcurrent_player] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100.0\u001b[39m\n",
      "File \u001b[0;32m~/git/AlphaBlokus/rl_agents/mcts.py:204\u001b[0m, in \u001b[0;36mstt_final_scores\u001b[0;34m(st)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstt_final_scores\u001b[39m(st: BoardState):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Helper to call compute_final_scores properly. Returns list of length 4.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgame_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_final_scores\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_final_scores(st)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AlphaBlokusNet()\n",
    "\n",
    "# Train\n",
    "train_alphablokus(model,\n",
    "                  device=\"cpu\",\n",
    "                  total_iterations=2,\n",
    "                  games_per_iteration=2,\n",
    "                  n_mcts_sim=2,\n",
    "                  batch_size=2,\n",
    "                  train_steps_per_iter=100,\n",
    "                  replay_buffer_max=50000,\n",
    "                  lr=1e-3)\n",
    "\n",
    "# Evaluate\n",
    "avg_score = evaluate_vs_random(model, n_games=5, device=\"cpu\")\n",
    "logging.info(f\"Evaluation done. Avg Score color0 = {avg_score}\")\n",
    "\n",
    "# Save model\n",
    "save_path = \"alphablokus_model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "logging.info(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphablokus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
